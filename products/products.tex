% This document is part of the GaussianProductRefactor project.
% Copyright 2019 the authors. All rights reserved.

% to-do
% -----
% - HOGG: make problem statement more well-posed.
% - HOGG: write or sketch proof of the solution.
% - HOGG: expand solution-notes bullet points into paragraphs.
% - APW: add a worked-out example with figures.
% - remove all HOGG and APW from the text.
% - HOGG: distinguish between FML and partially marginalized likelihood:
%   sometimes you have parameters that enter non-linearly, and that's ok
% - should we say "positive semi-definite" instead of "non-negative semi-definite"?
% - audit all style notes
% - fix typesetting for "canonical form" of normal in final section

% style notes -- audit for these.
% -------------------------------
% - no condescending language; no "it is easy to show" or equivalent...!
% - use \vA not A or \vector{A} and so on.
% - modify typesetting above the \begin{document} line, not below.
% - distinguish types (vector, matrix, tensor).
% - never say ``the pdf for $\vy$''; instead say ``the pdf for the data $\vy$''.
% - ...[your pet peeve here]...

\documentclass[12pt, letterpaper]{article}
\usepackage{amsmath, bm, mathrsfs}

\usepackage[utf8]{inputenc}
\inputencoding{latin1}
\inputencoding{utf8}
\usepackage{pifont}

% text stuhh
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}
\newcommand{\acronym}[1]{\small{#1}}
\newcommand{\FML}{\acronym{FML}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}

% math stuhh
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\!\mathsf{T}\!}}
\newcommand{\inv}{^{-1}}
\newcommand{\scalar}[1]{#1}
\renewcommand{\vector}[1]{\boldsymbol{#1}}
\newcommand{\tensor}[1]{\mathbf{#1}}
\renewcommand{\matrix}[1]{\mathsf{#1}}
% \setlength{\fboxsep}{0.8pt}
% \newcommand{\blob}[1]{\boxed{#1}} % THIS IS HERE TO ANNOY APW
\newcommand{\blob}[1]{\mathscr{#1}} % THIS IS HERE TO ANNOY APW
\newcommand{\normal}{\mathcal{N}\!\,}

% variables
\newcommand{\va}{\vector{a}}
\newcommand{\vb}{\vector{b}}
\newcommand{\vm}{\vector{m}}
\newcommand{\vx}{\vector{x}}
\newcommand{\vy}{\vector{y}}
\newcommand{\vz}{\vector{z}}
\newcommand{\vmu}{\vector{\mu}}
\newcommand{\veta}{\vector{\eta}}
\newcommand{\vtheta}{\vector{\theta}}
\newcommand{\tA}{\tensor{A}}
\newcommand{\tB}{\tensor{B}}
\newcommand{\tC}{\tensor{C}}
\newcommand{\tD}{\tensor{D}}
\newcommand{\tI}{\tensor{I}}
\newcommand{\tQ}{\tensor{Q}}
\newcommand{\tS}{\tensor{S}}
\newcommand{\tH}{\tensor{H}}
\newcommand{\tLambda}{\tensor{\Lambda}}
\newcommand{\mM}{\matrix{M}}
\newcommand{\mN}{\matrix{N}}
\newcommand{\mU}{\matrix{U}}
\newcommand{\mV}{\matrix{V}}
% \newcommand{\bP}{\blob{P}}
% \newcommand{\bH}{\blob{H}}
% This is the star:
% \newcommand{\bP}{\ensuremath{\textrm{\ding{87}}}} % THIS IS HERE TO ANNOY DWH
\newcommand{\bP}{\ensuremath{\textrm{\ding{170}}}} % THIS IS HERE TO ANNOY DWH
\newcommand{\bH}{\ensuremath{\textrm{\ding{114}}}} % THIS IS HERE TO ANNOY DWH

% typesetting stuff
\addtolength{\topmargin}{-0.75in}
\addtolength{\textheight}{1.5in}
\setlength{\parindent}{\baselineskip}
\raggedbottom\sloppy\sloppypar\frenchspacing


\usepackage{color}
\newcommand{\bl}[1]{\textcolor{red}{[BL: #1]}}

\begin{document}

\section*{Data Analysis Recipes:\\ Products of dependent multivariate Gaussians}

\noindent\textbf{David W. Hogg}\footnote{%
The authors would like to thank
  Dan Foreman-Mackey (Flatiron) and
  Sam Roweis (deceased),
for help with all these concepts through the years.
This work was supported in part by the National Science Foundation
and the National Aeronautics and Space Administration.
}\\
{\footnotesize%
  \textsl{Center for Cosmology and Particle Physics, Department of Physics, New York University}\\
  \textsl{Max-Planck-Institut f\"ur Astronomie, Heidelberg}\\
  \textsl{Flatiron Institute, a division of the Simons Foundation}%
}

\medskip\noindent\textbf{Adrian Price-Whelan}\\
{\footnotesize%
  \textsl{Flatiron Institute, a division of the Simons Foundation}%
}

\medskip\noindent\textbf{Boris Leistedt}\\
{\footnotesize%
  \textsl{Department of Physics, Imperial College, London}\\
  \textsl{Center for Cosmology and Particle Physics, Department of Physics, New York University}%
}

\paragraph{Abstract:}
A product of two Gaussians is another Gaussian.
That's a valuable and useful fact!
Here we use it to derive a refactoring of a common product of
Gaussians:
The product of a Gaussian likelihood (probability for data given
parameters) times a Gaussian prior (probability for parameters),
where some (or all) of those parameters enter the likelihood
only in the mean (or expectation) and only linearly.
That is, a linear model, with a Gaussian likelihood and a Gaussian
prior.
This product of a likelihood times a prior can be refactored into a
product of a Bayesian evidence (or marginalized likelihood)
times a posterior, where both of these are also Gaussian.
Here we deliver the details of this refactorization.
We also explain how it connects to inferences that arise frequently in
physics and astronomy.

If all you want is the answer, the question is posed and answered
at the beginning of \sectionname~\ref{sec:problemsolution}.

\section{Inferences with linear parameters}

It is not uncommon in physics and astronomy that likelihood functions
(probabilities of data given parameters) are chosen to be Gaussian:
One reason is that a likelihood function is basically a noise model,
and it is often case that the noise is treated as Gaussian.
This Gaussian assumption for the likelihood function is
\emph{accurate} when the noise model has benefitted from the central
limit theorem.
This is true, for example, when the noise is thermal, or when the
noise is shot noise and the numbers (numbers of detected photons or other
particles) are large.
Another reason that the likelihood function is often treated as
Gaussian is that Gaussians are generally \emph{tractable}:
Many computations we like to perform on Gaussians, like integrals and
derivatives and optimizations, have closed-form solutions.
Even when we don't use the closed-form solutions, there are many
contexts in which Gaussians lead to convex functions,
providing guarantees to resulting inferences.

It is also not uncommon in physics and astronomy that models for data
include parameters such that the expectation value for the data (in,
say, a set of repeated experiments) is linearly proportional to some
subset of the parameters.
This is true, for example, when we fit a histogram of LHC events
affected by the Higgs boson; the expected number of counts in each
energy bin is proportional to a linear combination of the amplitudes
of various backgrounds and some coupling to the Higgs.
This is true, for example, when we fit for the radial-velocity
variation of a star in response to an faint, orbiting companion (the
problem we will use as an example here), where the expectation of the
radial-velocity measurements depends linearly on the binary system
velocity and some combination of masses and system inclination (with
respect to the line of sight).
In both of these cases, there are both linear parameters (like the
amplitudes) and non-linear parameters (like the mass of the Higgs, or
the period of the binary).
In what follows, we will spend our energies on the linear parameters,
though our work on them is in service of learning the non-linear
parameters too, of course.

Bayes theorem is often written as a ratio of probability density
functions (pdfs in what follows), but it can be written as a simple
pdf factorization:
\begin{equation}
p(\vy,\vtheta\given\bH) = p(\vy\given\vtheta,\bH)\,p(\vtheta\given\bH) = p(\vtheta\given\vy,\bH)\,p(\vy\given\bH)
\end{equation}
where
$p(\vy,\vtheta\given\bH)$ is the joint probability of data $\vy$ and
parameters $\vtheta$ given your model assumptions and hyper parameters
(symbolized jointly as $\bH$)\footnote{%
\textsl{Typographical note:} In this \documentname, we
typeset vectors (which are column vectors) as $\va, \vb, \vtheta$, we typeset tensors (which
are square, non-negative semi-definite matrices) as $\tC, \tLambda$,
we typeset matrices (which will in general be non-square) as $\mM, \mU$,
and we typeset blobs or unstructured
collections of information as $\bH, \bP$.},
$p(\vy\given\vtheta,\bH)$ is the likelihood, or probability of data $\vy$
given parameters (and assumptions),
$p(\vtheta\given\bH)$ is the prior pdf for the parameters $\vtheta$,
$p(\vtheta\given\vy,\bH)$ is the posterior pdf for the parameters $\vtheta$
given the data,
and
$p(\vy\given\bH)$ is the pdf for the data, marginalizing out all
parameters (sometimes called the Bayesian evidence or the fully
marginalized likelihood or \FML).

If the likelihood is Gaussian, and the expectation of the data depends linearly
on the parameters, and if we choose the prior pdf to also be Gaussian, then
eveything else (the joint, the posterior, and the \FML) becomes Gaussian too.
The main point of this \documentname\ is that the means and variances of these
five Gaussians are all related by simple, closed-form expressions, given below.
One consequence of this math is that \emph{if} you have a Gaussian
likelihood function, and \emph{if} you have a subset of parameters
that are linearly related to the expectation of the data, \emph{then}
you can obtain both the posterior pdf and the \FML\ with just simple
closed-form transformations of the means and variances of the
likelihood and prior pdf.

\section{Marginalization by refactorization}

Imagine that we are doing an inference using data $\vy$ (which is a
$N$-dimensional vector, say).
We are trying to learn parameters $\vtheta$ and also $\bP$ (the former a
$K$-dimensional vector and the latter an arbitrary vector, list,
or blob.
Whether we are Bayesian or frequentist, the inference is based on
a likelihood function, or probability for the data given parameters
\begin{equation}
\mbox{likelihood} \quad p(\vy\given\vtheta,\bP)
~ .
\end{equation}

Now let's imagine that the parameters $\vtheta$ are either nuisance
parameters, or else easily marginalized, so we want to marginalize
them out.
This will leave us with a lower-dimensional marginalized likelihood
function
\begin{equation}
\mbox{marginalized likelihood} \quad p(\vy\given\bP)
~ .
\end{equation}
That's good, but the marginalization comes at a cost:
We have to choose a prior
\begin{equation}
\mbox{prior on nuisances} \quad p(\vtheta\given\bP)
\end{equation}
on the nuisances.
This is the basis for the claim (stated elsewhere; HOGG) that
inference requires a likelihood function, and priors on the nuisances.
It does not require a prior on everything, contrary to some statements
in the pedagogical literature (for example, HOGG).
We have said ``$p(\vtheta\given\bP)$'' because this prior may depend on
the parameters $\bP$. But it certainly doesn't have to.

To perform the marginalization, we have two choices.
We can either do an integral:
\begin{equation}
p(\vy\given\bP) = \int p(\vy\given\vtheta,\bP)\,p(\vtheta\given\bP)\,\dd\vtheta
~ ,
\end{equation}
where the integral is implicitly over the entire domain of the
nuisances $\vtheta$ (or the entire support of the prior).
Or we can do a clever re-factorization of this form \bl{Isn't just Bayes theorem again? I wouldn't say a re-factorization, but rather simply emphasize that the posterior and evidence have closed form solutions.}:
\begin{equation}
p(\vy\given\vtheta,\bP)\,p(\vtheta\given\bP)
 = p(\vtheta\given\vy,\bP)\,p(\vy\given\bP)
~ .
\end{equation}
That is, in certain magical circumstances it is possible to do this
re-factorization without explicitly doing any integral.
When this is true, the marginalization is sometimes far easier than
the relevant integral.

One such magical circumstance can arise when the two probability
distributions---the likelihood and the prior---are both Gaussian in
form, and when the model is linear.
In detail we will assume
\begin{enumerate}
\item
the likelihood $p(\vy\given\vtheta,\bP)$ is a Gaussian in $\vy$,
\item
the prior $p(\vtheta\given\bP)$ is a Gaussian in $\vtheta$,
\item
the mean of the likelihood Gaussian depends linearly on the nuisance
parameters $\vtheta$, and
\item
the nuisance parameters $\vtheta$ don't enter the likelihood anywhere
other than in the mean.
\end{enumerate}
In equations, this becomes:
\begin{equation}
p(\vy\given\vtheta,\bP) = \normal(\vy\given\mM\cdot\vtheta,\tC)
\end{equation}
\begin{equation}
p(\vtheta\given\bP) = \normal(\vtheta\given\vmu,\tLambda)
~ ,
\end{equation}
where
$\normal(\vx\given\vm,\tLambda)$ is the multivariate Gaussian pdf for a vector $\vx$
given a mean vector $\vm$ and a variance tensor $\tLambda$,
$\mM$ is a $N\times K$ rectangular design matrix (which depends, in
general, on the non-nuisance parameters $\bP$),
$\tC$ is a $N\times N$ covariance matrix of uncertainties for the
data (diagonal if the data dimensions are independent).
That is, the likelihood is a Gaussian with a mean that depends
linearly on the nuisance parameters $\vtheta$, and
$\vmu$ and $\tLambda$ are the $K$-vector mean and $K\times K$ variance tensor
for the Gaussian prior.

In this incredibly restrictive---but also surprisingly
common---situation, the re-factored pdfs $p(\vtheta\given\vy,\bP)$
(also known as the posterior for the linear parameters, conditioned on
the nonlinear parameters in $\bP$) and $p(\vy\given\bP)$ (the
partially marginalized likelihood, marginalizing out the linear
parameters) will also both be Gaussian.
Obtaining the specific forms of these Gaussians is the object of this
\documentname.

\section{Products of two Gaussians}\label{sec:problemsolution}

On the internets, there are many documents, slide decks, and videos
that explain products of Gaussians in terms of other Gaussians.
The vast majority of these consider either the univariate case (where
the data $\vy$ and the parameter $\vtheta$ are both simple scalars, which
is not useful for our science cases), or the same-dimension case (where the data
$\vy$ and the parameter vector $\vtheta$ are the same length, which never
occurs in our applications).
Here we solve this problem in the general case:
The inputs are multivariate (vectors) and the two Gaussians we are
multiplying live in spaces of different dimensions.
That is, we solve the following problem:

\paragraph{Problem:}
Find $N$-vector $\va$, $N\times N$ variance tensor $\tA$, $K$-vector $\vb$,
and $K\times K$ variance tensor $\tB$ such that
\begin{equation}\label{eq:problem}
\normal(\vy\given\mM\cdot\vtheta,\tC)\,\normal(\vtheta\given\vmu,\tLambda)
 = \normal(\vtheta\given\va,\tA)\,\normal(\vy\given\vb,\tB) ~ ,
\end{equation}
and such that $\va$, $\tA$, $\vb$, and $\tB$ don't depend on $\vtheta$ at all.
Note that
$\vy$ is a $N$-vector,
$\mM$ is a $N\times K$ matrix,
$\vtheta$ is a $K$-vector,
$\tC$ is a $N\times N$ non-negative semi-definite variance tensor,
$\vmu$ is a $K$-vector,
and
$\tLambda$ is a $K\times K$ non-negative semi-definite variance tensor.

\paragraph{Solution:}
\begin{equation}
\tA\inv = \tLambda\inv + \mM\T \cdot \tC\inv \cdot \mM
\end{equation}
\begin{equation}
\va = \tA \cdot (\tLambda\inv \cdot \vmu + \mM\T \cdot \tC\inv \cdot \vy)
\end{equation}
\begin{equation}
\tB = \tC + \mM \cdot \tLambda \cdot \mM\T
\end{equation}
\begin{equation}
\vb = \mM \cdot \vmu
~ .
\end{equation}
This is the complete solution to the problem, and constitutes the main point
of this \documentname.
For completeness, we will give some discussion!

\paragraph{Proof:}
The two sides of equation~(\ref{eq:problem}) are identical if two things hold.
The first thing is that the determinant products must be equal:
\begin{equation}
||\tC||\,||\tLambda|| = ||\tA||\,||\tB||
~ ,
\end{equation}
because the determinants are involved in the normalizations of the
functions.
This equality of determinant products follows straightforwardly from
the matrix determinant lemma
\begin{equation}\label{eq:detlemma}
||\tQ + \mU\cdot\mV\T|| = ||\tI + \mV\T\cdot\tQ\inv\cdot\mU||\,||\tQ||
~ ,
\end{equation}
where $\mU$ and $\mV$ can be rectangular, and $\tI$ is the correct-sized identity matrix.
This identity implies that
\begin{equation}
||\tA\inv|| = ||\tI + \mM\T\cdot\tC\inv\cdot\mM\cdot\tLambda||\,||\tLambda\inv||
\end{equation}
\begin{equation}
||\tB||     = ||\tI + \mM\T\cdot\tC\inv\cdot\mM\cdot\tLambda||\,||\tC||
~,
\end{equation}
where we had to apply the identity twice to get the $||\tA\inv||$ expression.
We can ratio these as follows to prove this first thing:
\begin{equation}
||\tA||\,||\tB||
 = \frac{||\tB||}{||\tA\inv||}
 = \frac{||\tC||}{||\tLambda\inv||}
 = ||\tC||\,||\tLambda||
~ .
\end{equation}

The second thing required for the proof is that the quadratic scalar form
\begin{equation}\label{eq:LHS}
[\vy-\mM\cdot\vtheta]\T\cdot\tC\inv\cdot[\vy-\mM\cdot\vtheta]
+ [\vtheta-\vmu]\T\cdot\tLambda\inv\cdot[\vtheta-\vmu]
\end{equation}
must equal the quadratic scalar form
\begin{equation}\label{eq:RHS}
[\vtheta-\va]\T\cdot\tA\inv\cdot[\vtheta-\va]
+ [\vy-\vb]\T\cdot\tB\inv\cdot[\vy-\vb]
~,
\end{equation}
because these quadratic scalar forms appear in the exponents in the functions.
This equality follows from straightforward expansion of
all the quadratic forms, plus some use of the matrix inversion lemma,
\begin{equation}\label{eq:invlemma}
[\tQ + \mU\cdot\tS\cdot\mV\T]\inv = \tQ\inv - \tQ\inv\cdot\mU\cdot[\tS\inv + \mV\T\cdot\tQ\inv\cdot\mU]\inv\cdot\mV\T\cdot\tQ\inv
~ ,
\end{equation}
which gives an expression for the inverse $\tB\inv$ of the \FML\ variance:
\begin{equation}
\tB\inv = \tC\inv - \tC\inv\cdot\mM\cdot[\tLambda\inv + \mM\T\cdot\tC\inv\cdot\mM]\inv\cdot\mM\T\cdot\tC\inv
~ .
\end{equation}
After that it's just a lot of grinding through matrix expressions.\footnote{%
We leave this grinding to the avid reader.
For guidance, it might help to realize that there are terms that
contain $\vtheta\T\cdots\vtheta$, $\vtheta\T\cdots\vy$, $\vy\T\cdots\vy$,
$\vtheta\T\cdots\vmu$, and $\vmu\T\cdots\vmu$.
If you expand out each of these five kinds of terms, each of the five
should lead to an indepenent-ish equality.}

\paragraph{Solution notes:}
In principle we found this factorization by expanding the quadratic in
(\ref{eq:LHS}) and then completing the square.
Of course we didn't really; we used arguments (which physicists love)
called \emph{detailed balance}:
We required that the terms that look like
$\vtheta\T\cdot\tQ\cdot\vtheta$ were equal between the LHS~(\ref{eq:LHS})
and the RHS~(\ref{eq:RHS}), and then all the terms that look like
$\vmu\T\cdot\tS\cdot\vmu$, and so on.
It turns out you don't have to consider them all to get the right solution.

Because the matrix $\mM$ is not square, it has no inverse. And because this
is a physics problem, it has units (which are the units of $\dd\vy/\dd\vtheta$).
It's beautiful in the solution that $\mM$ and $\mM\T$ appear only where the
units make sense.
They make sense because the units of $\tC$ are inverse data-squared (where $\vy$
is the data vector) and the units of $\tLambda$ are inverse parameters-squared.
And they are different sizes.

Various parts of the solution are highly interpretable in terms of the
objects of Bayesian inference. For example, the vector $\va$ is the
maximum \foreign{a posteriori} (or \acronym{MAP}) value for the parameter vector
$\vtheta$.
It is found by inverse-variance-weighted combinations of the data and the prior.
\bl{I would say this is worth highlighting more since there are definitely some problems where this MAP interpretation is useful and could be used elsewhere.}
The variance tensor $\tA$ is the posterior variance in the parameter space.
It is strictly non-larger (in eigenvalues or determinant) than either
the prior variance $\tLambda$ or the parameter-space data-noise
variance $\mM\T\cdot\tC\cdot\mM$.
The vector $\vb$ is the prior-optimal (maximum \foreign{a priori})
value for the data $\vy$.
It is the most likely data under the prior pdf.
And the variance tensor $\tB$ is the prior variance expanded out to the
data space, and including the noise variance in the data.
It is strictly non-smaller than both the data noise variance $\tC$ and the
data-space prior variance $\mM\cdot\tLambda\cdot\mM\T$.

HOGG: Comment on the possibility that there is a solution that flows
from thinking about the product of Gaussians as one Gaussian for the
joint. Does that lead to a simpler discussion?

\paragraph{Implementation notes:}
\bl{If this paragraph gets any bigger I would break it into bullet points, and say more about the necessity of using the matrix inversion lemma for numerical stability, especially when the dimensions differ greatly (M "very non square").}
The solution gives an expression for the variance tensor $\tB$, but
note that when you actually evaluate the pdfs you probably need to
have either the inverse of $\tB$, or else an operator that computes
the product of the inverse and vectors, as in $\tB\inv\cdot\vy$ and
the same for $\vb$.
To get the inverse of the tensor $\tB$ stably, \emph{you might want to use
the matrix inversion lemma} (\ref{eq:invlemma}) given above.
This is often useful because you often know the data noise-variance
tensor inverse $\tC\inv$, and the prior variance inverse
$\tLambda\inv$.

We also give the general advice that one should \emph{never take an explicit
inverse} (unless you know the inverse exactly in closed form, as you do
for, say, diagonal tensors).
In your code you should never use the \code{inv()} function; you
should always use a \code{solve()} function.
The reason is that the code operation \code{inv(B)} returns the best
possible inverse to machine precision (if you are lucky), but what you
really want instead is the best possible product of that inverse times
a vector.
So in general \code{solve(B,y)} will deliver more precise results than
the mathematically equivalent \code{dot(inv(B),y)}.

The expressions in (\ref{eq:problem}) do not require that the tensors
$\tC$, $\tLambda$, $\tA$, $\tB$ be positive definite; they only require
that they be non-negative semi-definite.
That means that they can have zero eigenvalues.
As can their inverses $\tC\inv$, $\tLambda\inv$, $\tA\inv$, $\tB\inv$.
If either of these might happen in your problem---like if your prior
freezes the parameters to a subspace of the $\vtheta$-space, which
would lead to a zero eigenvalue in $\tLambda$, or if a data point is
unmeasured or missing, which would lead to a zero eigenvalue in
$\tC\inv$---you might have to \emph{think about how you implement the
linear algebra operations to be zero-safe}.


\paragraph{Simplification: single multiplicative scaling}

One interesting case is when $K=1$, so the design matrix in fact reduces to a model vector $\vm$, multiplied by a scalar $\theta$, and $\mu$ and $\Lambda$ are now scalars as well.

\begin{equation}
\mbox{likelihood} \times \mbox{prior} \quad \normal(\vy\given\vm\cdot\theta,\tC)\normal(\theta\given \mu, \Lambda) = \normal(\theta\given a,A)\,\normal(\vy\given\vb,\tB)
~ .
\end{equation}

In this case, the previous equations are simplified and no longer involve as matrix matrix operations (in particular inversions), which results in them being typically more numerically stable (in which case the matrix inversion lemma reduces to the Sherman–Morrison formula, applicable to derive $A$ and $\tB$).




\section{Worked Example}

When working with a probabilistic model that meets the strong requirements
imposed above (Gaussians everywhere), the identities described in this
\documentname\ have a practical use: To reduce the dimensionality of your model.
This is often desired when your inferences depend on generating posterior
samples of your model parameters.
Reducing the dimensionality of your parameter-space will in general improve
convergence of Markov Chain Monte Carlo (MCMC) sampling methods, or enable
alternate sampling methods (e.g., rejection sampling) that may be intractable
when the parameter dimensionality is large.

Below, we demonstrate one way in which these identities can be used and
incorporated into statistical inferences TODO APW.
% One case where the parameter dimensionality is reduced, and another case where
% standard MCMC is intractable but the straightforward evaluation of the
% marginal likelihood enables using brute-force rejection sampling to generate
% posterior samplings.

APW: other notes as footnotes. (1) Like you wouldn't turn on scipy.optimize to get your maximum-likelihood linear parameters, don't turn on MCMC to generate samples if everything is linear and gaussian, (2) Another footnote or note about how this enables other sampling methods like The Joker.

APW: Example: Fit a spectral line + background. Reduced dimensionality from 5 to 2 parameters, run with emcee.

\bl{Simple example would be to marginalize over a couple of nuisance parameters, say additive nuisance polynomials on top of a model of interest, for SDSS stars or galaxies for example?}

\section{Generalization: product of many Gaussians}

An interesting case that may arise in some applications is when the likelihood is made of multiple Gaussian terms, for example when including independent subsets of data points in an analysis. This results in having to treat the product of many Gaussians, with arbitrary design matrices, which we can write as
\begin{equation}\label{eq:generalgaussianproduct}
\mbox{likelihood}\times\mbox{prior:} \quad \prod_i \normal(  \vmu_i \given \mM_i\cdot\vtheta ,\tLambda_i)
~ .
\end{equation}

For the specific case of two subsets of data points, this could look like
\begin{equation}
\mbox{likelihood} \quad \normal(\vy\given\mM\cdot\vtheta,\tC) \ \normal(\vz\given\mN\cdot\vtheta,\tD)
\end{equation}
\begin{equation}
\mbox{prior} \quad \normal(\vtheta\given\vmu,\tLambda)
~ .
\end{equation}
This is trivial if the design matrices are identical, $\mM = \mN$, since one can rewrite the likelihood as a single multivariate Gaussian (concatenating the vectors $\vy$ and $\vz$ and forming a block diagonal matrix with $\tC$ and $\tD$) and apply the formulae derived previously. But for $\mM \neq \mN$, this does not apply. For a fixed number of terms, the techniques from above (completing the square and detailed balance) could be used but become cumbersome. We will derive a general formula using \textit{the canonical form of Gaussian distributions}.
For a multivariate Gaussian $\normal(\vx\given\vm,\tLambda)$, the canonical form is
\begin{equation}
\normal^{\ -1}(\vx\given\veta, \tH) = \exp\left(\xi +  \veta^T\cdot\vx - \frac{1}{2}\vx^T \cdot\tH \cdot \vx\right)
~ ,
\end{equation}
The parameters are obtained by equating the canonical form with the standard form, $\tH = \tLambda^{\ -1}$, $\veta = \tH\cdot \vm = \tLambda^{-1}\cdot \vm$, and the normalization
$\xi = -\frac{1}{2}\left( d\log 2\pi -\log|\tH| + \veta^T \cdot \tH\inv\cdot \veta \right)$
with $d$ the dimensionality of $\vx$.

Equipped with the canonical form, it is easy to see that the product of any number of Gaussians will reduce to the product of two terms: one with all of the terms in $\veta$ and $\vx$, and the other one the normalization necessary to turn the first term into a valid canonical form. We will apply this idea to (\ref{eq:generalgaussianproduct}):

\begin{eqnarray}
\prod_i \normal( \mM_i\cdot\vtheta \given \vmu_i,\tLambda_i) &=& \prod_i \normal^{\ -1}( \mM_i\cdot\vtheta\given\veta_i, \tH_i) \\
&=&   \underbrace{\normal^{\ -1}(\vtheta\given\bar{\veta}, \bar{\tH})}_{ \normal(\vtheta\given\va,\tA) } \ \times \ \exp\left(\textstyle{\sum_i} \xi_i - \bar{\xi} \right)
\end{eqnarray}
naturally grouping the terms depending on $\vtheta$ and following the convention of the previous section.


The parameters of the individual Gaussians in canonical form are
\begin{eqnarray}
\veta_i &=& \tLambda_i^{-1} \cdot \vmu_i \\
\tH_i  &=& \tLambda_i^{-1}\\
\xi_i &=& -\frac{1}{2}\left( d_i \log 2\pi -\log|\tH_i| + \veta_i^T \cdot \tH_i\inv\cdot \veta_i \right)
\end{eqnarray}
where $d_i$ is the dimensionality of $\vmu_i$, and that of the total Gaussian are
\begin{eqnarray}
\bar{\veta} &=& \sum_i \mM_i \cdot \veta_i = \sum_i \mM_i \cdot  \tLambda_i^{-1} \cdot \vmu_i  \\
\bar{\tH} &=& \sum_i  \mM_i^T\cdot \tH_i\cdot \mM_i = \sum_i  \mM_i^T\cdot \tLambda_i^{-1}\cdot \mM_i \\
 \bar{\xi} &=&  -\frac{1}{2}\left( N\log 2\pi -\log|\bar{\tH} | + \bar{\veta}^T \cdot \bar{\tH}\inv\cdot \bar{\veta} \right)\\
\va &=&  \bar{\tH}^{-1} \bar{\xi} \\
\tA &=& \bar{\tH}^{-1}
\end{eqnarray}
where $N$ is the dimensionality of $\vtheta$.
Equipped with this result, we can re-derive the formulae of the previous sections.
We can again identify $\va$ as the maximum a posteriori value for $\vtheta$, and $\tA$ the associated Gaussian covariance.
Furthermore, the term $ \normal(\vtheta\given\va,\tA) $ reduces to one when marginalizing over $\vtheta$, and we are left with the \FML, $ \exp\left(\textstyle{\sum_i} \xi_i - \bar{\xi} \right)$.

One culprit of this approach is that its implementation. Indeed, it involves inverting the individual covariances $\tLambda_i$, projecting them with $\mM_i$, in order to form $\bar{\tH}$ and  $\bar{\veta}$ and apply them onto each other to obtain $\va$, $\tA$ and $ \bar{\xi}$. Depending on the condition numbers of the square matrices, and the dimensionality of the projection matrices, applying those formulae may be numerically unstable. In practice, this could simply result from the noise levels in the data and how the model is projected from the parameters $\vtheta$ onto the data.
It is therefore highly recommended to apply those formulae with care, and verify the stability of the results.
If matrices with bad condition numbers are involved, one solution would be to apply the matrix inversion lemma. Unfortunately this would only apply to one term $\mM_i^T\cdot \tLambda_i^{-1}\cdot \mM_i$ at a time, and it may be necessary to apply it multiple times, resulting in long expressions (that lead to numerically stable results).
Since this is application-specific, we do not derive those expressions here.





\end{document}

\documentclass[12pt, letterpaper]{article}

% text stuhh
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\acronym}[1]{\small{#1}}
\newcommand{\FML}{\acronym{FML}}

% math stuhh
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\!\mathsf{T}}}
\newcommand{\inv}{^{-1}}

% typesetting stuff
\addtolength{\topmargin}{-0.75in}
\addtolength{\textheight}{1.5in}
\raggedbottom\sloppy\sloppypar\frenchspacing

\begin{document}

\section*{Products of dependent multivariate Gaussians}

\noindent
\textbf{David W. Hogg}\\
{\footnotesize%
  \textsl{Center for Cosmology and Particle Physics, Department of Physics, New York University}\\
  \textsl{Max-Planck-Institut f\"ur Astronomie, Heidelberg}\\
  \textsl{Flatiron Institute, a division of the Simons Foundation}%
}

\noindent
\textbf{Adrian Price-Whelan}\\
{\footnotesize%
  \textsl{Flatiron Institute, a division of the Simons Foundation}%
}

\paragraph{Abstract:}
A product of two Gaussians is another Gaussian.
That's a valuable and useful fact!
Here we use it to derive a refactoring of a common product of
Gaussians:
The product of a Gaussian likelihood (probability for data given
parameters) times a Gaussian prior (probability for parameters),
where some (or all) of those parameters enter the likelihood
only in the mean (or expectation) and only linearly.
That is, a linear model, with a Gaussian likelihood and a Gaussian
prior.
This product of a likelihood times a prior can be refactored into a
product of a Bayesian evidence (or fully marginalized likelihood)
times a posterior, where both of these are also Gaussian.
Here we deliver the details of this refactorization.
We also explain how it connects to inferences that arise frequently in
physics and astronomy.

\section{Inferences with linear parameters}

It is not uncommon in physics and astronomy that likelihood functions
(probabilities of data given parameters) are chosen to be Gaussian:
One reason is that a likelihood function is basically a noise model,
and it is often case that the noise is treated as Gaussian.
This Gaussian assumption for the likelihood function is
\emph{accurate} when the noise model has benefitted from the central
limit theorem.
This is true, for example, when the noise is thermal, or when the
noise is shot noise and the number of detected photons (or other
particles) is large.
Another reason that the likelihood function is often treated as
Gaussian is that Gaussians are generally \emph{tractable}:
Many computations we like to perform on Gaussians, like integrals and
derivatives and optimizations, have closed-form solutions.
Even when we don't use the closed-form solutions, there are many
spaces in which Gaussians are technically convex, providing simplicity
and guarantees to inferences.

It is also not uncommon in physics and astronomy that models for data
include parameters such that the expectation value for the data (in,
say, a set of repeated experiments) is linearly proportional to some
subset of the parameters.
This is true, for example, when we fit a histogram of LHC events
affected by the Higgs boson; the expected number of counts in each
energy bin is proportional to a linear combination of the amplitudes
of various backgrounds and some coupling to the Higgs.
This is true, for example, when we fit for the radial-velocity
variation of a star in response to an faint, orbiting companion (the
problem we will use as an example here), where the expectation of the
radial-velocity measurements depends linearly on the binary system
velocity and some combination of masses and system inclination (with
respect to the line of sight).
In both of these cases, there are both linear parameters (like the
amplitudes) and non-linear parameters (like the mass of the Higgs, or
the period of the binary).
In what follows, we will spend our energies on the linear parameters,
though our work on them is in service of learning the non-linear
parameters too, of course.

Bayes theorem is often written as a ratio of probability density
functions (pdfs in what follows), but it can be written as a simple
pdf factorization:
\begin{equation}
p(y,\theta\given H) = p(y\given\theta,H)\,p(\theta\given H) = p(\theta\given y,H)\,p(y\given H)
\end{equation}
where
$p(y,\theta\given H)$ is the joint probability of data $y$ and
parameters $\theta$ given your model assumptions and hyper parameters
(symbolized jointly as $H$),
$p(y\given\theta,H)$ is the likelihood, or probability of data $y$
given parameters (and assumptions),
$p(\theta\given H)$ is the prior pdf for the parameters $\theta$,
$p(\theta\given y,H)$ is the posterior pdf for the parameters $\theta$
given the data,
and
$p(y\given H)$ is the pdf for the data, marginalizing out all
parameters (sometimes called the Bayesian evidence or the fully
marginalized likelihood or \FML).

The main point of this \documentname\ is that if the likelihood is
Gaussian, and the expectation of the data depends linearly on the
parameters, and if we choose the prior pdf to also be Gaussian, then
eveything else (the joint, the posterior, and the \FML) becomes
Gaussian too.
And, moreover, the means and variances of these five Gaussians are all
related by simple, closed-form expressions.
One consequence of this math is that \emph{if} you have a Gaussian
likelihood function, and \emph{if} you have a subset of parameters
that are linearly related to the expectation of the data, \emph{then}
you can obtain both the posterior pdf and the FML with just simple
closed-form transformations of the means and variances of the
likelihood and prior pdf.

\section{Marginalization by refactorization}

Imagine that we are doing an inference using data $y$ (which is a
$N$-dimensional vector, say).
We are trying to learn parameters $\theta$ and also $P$ (the former a
$K$-dimensional vector and the latter an arbitrary vector, list,
or blob.
Whether we are Bayesian or frequentist, the inference is based on
a likelihood function, or probability for the data given parameters
\begin{equation}
\mbox{likelihood} \quad p(y\given\theta,P)
\quad .
\end{equation}

Now let's imagine that the parameters $\theta$ are either nuisance
parameters, or else easily marginalized, so we want to marginalize
them out.
This will leave us with a lower-dimensional marginalized likelihood
function
\begin{equation}
\mbox{marginalized likelihood} \quad p(y\given P)
\quad .
\end{equation}
That's good, but the marginalization comes at a cost:
We have to choose a prior
\begin{equation}
\mbox{prior on nuisances} \quad p(\theta\given P)
\end{equation}
on the nuisances.
This is the basis for the claim (stated elsewhere; HOGG) that
inference requires a likelihood function, and priors on the nuisances.
It does not require a prior on everything, contrary to some statements
in the pedagogical literature (for example, HOGG).
We have said ``$p(\theta\given P)$'' because this prior may depend on
the parameters $P$. But it certainly doesn't have to.

To perform the marginalization, we have two choices.
We can either do an integral:
\begin{equation}
p(y\given P) = \int p(y\given\theta, P)\,p(\theta\given P)\,\dd\theta
\quad ,
\end{equation}
where the integral is implicitly over the entire domain of the
nuisances $\theta$ (or the entire support of the prior).
Or we can do a clever re-factorization of this form:
\begin{equation}
p(y\given\theta,P)\,p(\theta\given P)
 = p(\theta\given y,P)\,p(y\given P)
\quad .
\end{equation}
That is, in certain magical circumstances it is possible to do this
re-factorization without explicitly doing any integral.
When this is true, the marginalization is sometimes far easier than
the relevant integral.

One such magical circumstance can arise when the two probability
distributions---the likelihood and the prior---are both Gaussian in
form, and when the model is linear.
In detail we will assume
\begin{enumerate}
\item
the likelihood $p(y\given\theta, P)$ is a Gaussian in $y$,
\item
the prior $p(\theta\given P)$ is a Gaussian in $\theta$,
\item
the mean of the likelihood Gaussian depends linearly on the nuisance
parameters $\theta$, and
\item
the nuisance parameters $\theta$ don't enter the likelihood anywhere
other than in the mean.
\end{enumerate}
In equations, this becomes:
\begin{equation}
p(y\given\theta, P) = N(y\given M\cdot\theta, C)
\end{equation}
\begin{equation}
p(\theta\given P) = N(\theta\given \mu, \Lambda)
\quad ,
\end{equation}
where
$N(x\given m,\Lambda)$ is the multivariate Gaussian pdf for a vector $x$
given a mean vector $m$ and a variance tensor $\Lambda$,
$M$ is a $N\times K$ rectangular design matrix (which depends, in
general, on the non-nuisance parameters $P$),
$C$ is a $N\times N$ covariance matrix of uncertainties for the
data (diagonal if the data dimensions are independent).
That is, the likelihood is a Gaussian with a mean that depends
linearly on the nuisance parameters $\theta$, and
$\mu$ and $\Lambda$ are the $K$-vector mean and $K\times K$ variance tensor
for the Gaussian prior.

In this incredibly restrictive---but also surprisingly
common---situation, the re-factored pdfs $p(\theta\given y, P)$ (also
known as the posterior) and $p(y\given P)$ (the marginalized
likelihood) will also both be Gaussian.
Obtaining the specific forms of these Gaussians is the object of this
\documentname.

\section{Products of Gaussians}

On the internets, there are many documents, slide decks, and videos
that explain products of Gaussians in terms of other Gaussians.
The vast majority of these consider either the univariate case (where
the data $y$ and the parameter $\theta$ are both simple scalars, which
is completely uninteresting), or the same-dimension case (where the data
$y$ and the parameter vector $\theta$ are the same length, which never
occurs in practice).
Here we solve this problem in the general case:
The inputs are multivariate (vectors) and the two Gaussians we are
multiplying live in spaces of different dimensions.
That is, we solve the following problem:

\paragraph{Problem:}
Find $N$-vector $a$, $N\times N$ variance tensor $A$, $K$-vector $b$,
and $K\times K$ variance tensor $B$ such that
\begin{equation}
N(y\given M\cdot\theta, C)\,N(\theta\given\mu, \Lambda)
 = N(\theta\given a, A)\,N(y\given b, B) \quad ,
\end{equation}
and such that $a$, $A$, $b$, and $B$ don't depend on $\theta$ at all.

\paragraph{Solution:}
\begin{equation}
A\inv = \Lambda\inv + M\T \cdot C\inv \cdot M
\end{equation}
\begin{equation}
a = A \cdot (\Lambda\inv \cdot \mu + M\T \cdot C\inv \cdot y)
\end{equation}
\begin{equation}
B = C + M \cdot \Lambda \cdot M\T
\end{equation}
\begin{equation}
b = M \cdot \mu
\quad .
\end{equation}

\paragraph{Solution notes:}
\begin{itemize}
\item
We found this by completing the square. Actually we didn't really; we just
used clever arguments.
\item
Check out both the units and the dimensions of $M$ and where it appears!
\item
Note that $a$ is the MAP value for the nuisance parameters $\theta$.
\item
Note that $b$ is the prior-optimal value for the data $y$.
\item
Check out how we are zero-safe in the variance tensors.
\item
Comment about determinants of the matrices?
\item
Note that when you actually compute these pdfs you might need to have the
inverse of $B$. To get that, you should use an inversion lemma, duh!
\end{itemize}

\end{document}

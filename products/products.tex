% This document is part of the GaussianProductRefactor project.
% Copyright 2019 the authors. All rights reserved.

% to-do
% -----
% - HOGG: make problem statement more well-posed.
% - HOGG: write or sketch proof of the solution.
% - HOGG: expand solution-notes bullet points into paragraphs.
% - APW: add a worked-out example with figures.
% - remove all HOGG and APW from the text.
% - HOGG: distinguish between FML and partially marginalized likelihood:
%   sometimes you have parameters that enter non-linearly, and that's ok

% style notes
% -----------
% - use \vA not A or \vector{A} and so on.
% - modify typesetting above the \begin{document} line, not below.
% - distinguish types (vector, matrix, tensor).
% - never say ``the pdf for $\vy$''; instead say ``the pdf for the data $\vy$''.
% - ...[your pet peeve here]...

\documentclass[12pt, letterpaper]{article}
\usepackage{bm, mathrsfs}

% text stuhh
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}
\newcommand{\acronym}[1]{\small{#1}}
\newcommand{\FML}{\acronym{FML}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}

% math stuhh
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\!\mathsf{T}\!}}
\newcommand{\inv}{^{-1}}
\newcommand{\scalar}[1]{#1}
\renewcommand{\vector}[1]{\boldsymbol{#1}}
\newcommand{\tensor}[1]{\mathbf{#1}}
\renewcommand{\matrix}[1]{\mathsf{#1}}
\newcommand{\blob}[1]{\underline{#1}} % THIS IS HERE TO ANNOY APW
\newcommand{\normal}{\mathcal{N}\!}

% variables
\newcommand{\va}{\vector{a}}
\newcommand{\vb}{\vector{b}}
\newcommand{\vm}{\vector{m}}
\newcommand{\vx}{\vector{x}}
\newcommand{\vy}{\vector{y}}
\newcommand{\vmu}{\vector{\mu}}
\newcommand{\vtheta}{\vector{\theta}}
\newcommand{\tA}{\tensor{A}}
\newcommand{\tB}{\tensor{B}}
\newcommand{\tC}{\tensor{C}}
\newcommand{\tI}{\tensor{I}}
\newcommand{\tQ}{\tensor{Q}}
\newcommand{\tS}{\tensor{S}}
\newcommand{\tLambda}{\tensor{\Lambda}}
\newcommand{\mM}{\matrix{M}}
\newcommand{\mU}{\matrix{U}}
\newcommand{\mV}{\matrix{V}}
\newcommand{\bP}{\blob{P}}
\newcommand{\bH}{\blob{H}}

% typesetting stuff
\addtolength{\topmargin}{-0.75in}
\addtolength{\textheight}{1.5in}
\setlength{\parindent}{\baselineskip}
\raggedbottom\sloppy\sloppypar\frenchspacing

\begin{document}

\section*{Data Analysis Recipes:\\ Products of dependent multivariate Gaussians}

\noindent\textbf{David W. Hogg}\footnote{%
The authors would like to thank
  Dan Foreman-Mackey (Flatiron) and
  Sam Roweis (deceased),
for help with all these concepts through the years.
This work was supported in part by the National Science Foundation
and the National Aeronautics and Space Administration.
}\\
{\footnotesize%
  \textsl{Center for Cosmology and Particle Physics, Department of Physics, New York University}\\
  \textsl{Max-Planck-Institut f\"ur Astronomie, Heidelberg}\\
  \textsl{Flatiron Institute, a division of the Simons Foundation}%
}

\medskip\noindent\textbf{Adrian Price-Whelan}\\
{\footnotesize%
  \textsl{Flatiron Institute, a division of the Simons Foundation}%
}

\medskip\noindent\textbf{Boris Leistedt}\\
{\footnotesize%
  \textsl{Imperial College, London}\\
  \textsl{Center for Cosmology and Particle Physics, Department of Physics, New York University}%
}

\paragraph{Abstract:}
A product of two Gaussians is another Gaussian.
That's a valuable and useful fact!
Here we use it to derive a refactoring of a common product of
Gaussians:
The product of a Gaussian likelihood (probability for data given
parameters) times a Gaussian prior (probability for parameters),
where some (or all) of those parameters enter the likelihood
only in the mean (or expectation) and only linearly.
That is, a linear model, with a Gaussian likelihood and a Gaussian
prior.
This product of a likelihood times a prior can be refactored into a
product of a Bayesian evidence (or marginalized likelihood)
times a posterior, where both of these are also Gaussian.
Here we deliver the details of this refactorization.
We also explain how it connects to inferences that arise frequently in
physics and astronomy.

If all you want is the answer, the question is posed and answered
at the beginning of \sectionname~\ref{sec:problemsolution}.

\section{Inferences with linear parameters}

It is not uncommon in physics and astronomy that likelihood functions
(probabilities of data given parameters) are chosen to be Gaussian:
One reason is that a likelihood function is basically a noise model,
and it is often case that the noise is treated as Gaussian.
This Gaussian assumption for the likelihood function is
\emph{accurate} when the noise model has benefitted from the central
limit theorem.
This is true, for example, when the noise is thermal, or when the
noise is shot noise and the numbers (numbers of detected photons or other
particles) are large.
Another reason that the likelihood function is often treated as
Gaussian is that Gaussians are generally \emph{tractable}:
Many computations we like to perform on Gaussians, like integrals and
derivatives and optimizations, have closed-form solutions.
Even when we don't use the closed-form solutions, there are many
contexts in which Gaussians lead to convex functions,
providing guarantees to resulting inferences.

It is also not uncommon in physics and astronomy that models for data
include parameters such that the expectation value for the data (in,
say, a set of repeated experiments) is linearly proportional to some
subset of the parameters.
This is true, for example, when we fit a histogram of LHC events
affected by the Higgs boson; the expected number of counts in each
energy bin is proportional to a linear combination of the amplitudes
of various backgrounds and some coupling to the Higgs.
This is true, for example, when we fit for the radial-velocity
variation of a star in response to an faint, orbiting companion (the
problem we will use as an example here), where the expectation of the
radial-velocity measurements depends linearly on the binary system
velocity and some combination of masses and system inclination (with
respect to the line of sight).
In both of these cases, there are both linear parameters (like the
amplitudes) and non-linear parameters (like the mass of the Higgs, or
the period of the binary).
In what follows, we will spend our energies on the linear parameters,
though our work on them is in service of learning the non-linear
parameters too, of course.

Bayes theorem is often written as a ratio of probability density
functions (pdfs in what follows), but it can be written as a simple
pdf factorization:
\begin{equation}
p(\vy,\vtheta\given\bH) = p(\vy\given\vtheta,\bH)\,p(\vtheta\given\bH) = p(\vtheta\given\vy,\bH)\,p(\vy\given\bH)
\end{equation}
where
$p(\vy,\vtheta\given\bH)$ is the joint probability of data $\vy$ and
parameters $\vtheta$ given your model assumptions and hyper parameters
(symbolized jointly as $\bH$)\footnote{%
\textsl{Typographical note:} In this \documentname, we
typeset vectors (which are column vectors) as $\va, \vb, \vtheta$, we typeset tensors (which
are square, non-negative semi-definite matrices) as $\tC, \tLambda$,
we typeset matrices (which will in general be non-square) as $\mM, \mU$,
and we typeset blobs or unstructured
collections of information as $\bH, \bP$.},
$p(\vy\given\vtheta,\bH)$ is the likelihood, or probability of data $\vy$
given parameters (and assumptions),
$p(\vtheta\given\bH)$ is the prior pdf for the parameters $\vtheta$,
$p(\vtheta\given\vy,\bH)$ is the posterior pdf for the parameters $\vtheta$
given the data,
and
$p(\vy\given\bH)$ is the pdf for the data, marginalizing out all
parameters (sometimes called the Bayesian evidence or the fully
marginalized likelihood or \FML).

The main point of this \documentname\ is that if the likelihood is
Gaussian, and the expectation of the data depends linearly on the
parameters, and if we choose the prior pdf to also be Gaussian, then
eveything else (the joint, the posterior, and the \FML) becomes
Gaussian too.
And, moreover, the means and variances of these five Gaussians are all
related by simple, closed-form expressions.
One consequence of this math is that \emph{if} you have a Gaussian
likelihood function, and \emph{if} you have a subset of parameters
that are linearly related to the expectation of the data, \emph{then}
you can obtain both the posterior pdf and the \FML\ with just simple
closed-form transformations of the means and variances of the
likelihood and prior pdf.

\section{Marginalization by refactorization}

Imagine that we are doing an inference using data $\vy$ (which is a
$N$-dimensional vector, say).
We are trying to learn parameters $\vtheta$ and also $\bP$ (the former a
$K$-dimensional vector and the latter an arbitrary vector, list,
or blob.
Whether we are Bayesian or frequentist, the inference is based on
a likelihood function, or probability for the data given parameters
\begin{equation}
\mbox{likelihood} \quad p(\vy\given\vtheta,\bP)
\quad .
\end{equation}

Now let's imagine that the parameters $\vtheta$ are either nuisance
parameters, or else easily marginalized, so we want to marginalize
them out.
This will leave us with a lower-dimensional marginalized likelihood
function
\begin{equation}
\mbox{marginalized likelihood} \quad p(\vy\given\bP)
\quad .
\end{equation}
That's good, but the marginalization comes at a cost:
We have to choose a prior
\begin{equation}
\mbox{prior on nuisances} \quad p(\vtheta\given\bP)
\end{equation}
on the nuisances.
This is the basis for the claim (stated elsewhere; HOGG) that
inference requires a likelihood function, and priors on the nuisances.
It does not require a prior on everything, contrary to some statements
in the pedagogical literature (for example, HOGG).
We have said ``$p(\vtheta\given\bP)$'' because this prior may depend on
the parameters $\bP$. But it certainly doesn't have to.

To perform the marginalization, we have two choices.
We can either do an integral:
\begin{equation}
p(\vy\given\bP) = \int p(\vy\given\vtheta,\bP)\,p(\vtheta\given\bP)\,\dd\vtheta
\quad ,
\end{equation}
where the integral is implicitly over the entire domain of the
nuisances $\vtheta$ (or the entire support of the prior).
Or we can do a clever re-factorization of this form:
\begin{equation}
p(\vy\given\vtheta,\bP)\,p(\vtheta\given\bP)
 = p(\vtheta\given\vy,\bP)\,p(\vy\given\bP)
\quad .
\end{equation}
That is, in certain magical circumstances it is possible to do this
re-factorization without explicitly doing any integral.
When this is true, the marginalization is sometimes far easier than
the relevant integral.

One such magical circumstance can arise when the two probability
distributions---the likelihood and the prior---are both Gaussian in
form, and when the model is linear.
In detail we will assume
\begin{enumerate}
\item
the likelihood $p(\vy\given\vtheta,\bP)$ is a Gaussian in $\vy$,
\item
the prior $p(\vtheta\given\bP)$ is a Gaussian in $\vtheta$,
\item
the mean of the likelihood Gaussian depends linearly on the nuisance
parameters $\vtheta$, and
\item
the nuisance parameters $\vtheta$ don't enter the likelihood anywhere
other than in the mean.
\end{enumerate}
In equations, this becomes:
\begin{equation}
p(\vy\given\vtheta,\bP) = \normal(\vy\given\mM\cdot\vtheta,\tC)
\end{equation}
\begin{equation}
p(\vtheta\given\bP) = \normal(\vtheta\given\vmu,\tLambda)
\quad ,
\end{equation}
where
$\normal(\vx\given\vm,\tLambda)$ is the multivariate Gaussian pdf for a vector $\vx$
given a mean vector $\vm$ and a variance tensor $\tLambda$,
$M$ is a $N\times K$ rectangular design matrix (which depends, in
general, on the non-nuisance parameters $\bP$),
$\tC$ is a $N\times N$ covariance matrix of uncertainties for the
data (diagonal if the data dimensions are independent).
That is, the likelihood is a Gaussian with a mean that depends
linearly on the nuisance parameters $\vtheta$, and
$\vmu$ and $\tLambda$ are the $K$-vector mean and $K\times K$ variance tensor
for the Gaussian prior.

In this incredibly restrictive---but also surprisingly
common---situation, the re-factored pdfs $p(\vtheta\given\vy,\bP)$ (also
known as the posterior) and $p(\vy\given\bP)$ (the marginalized
likelihood) will also both be Gaussian.
Obtaining the specific forms of these Gaussians is the object of this
\documentname.

\section{Products of Gaussians}\label{sec:problemsolution}

On the internets, there are many documents, slide decks, and videos
that explain products of Gaussians in terms of other Gaussians.
The vast majority of these consider either the univariate case (where
the data $\vy$ and the parameter $\vtheta$ are both simple scalars, which
is not useful for our science cases), or the same-dimension case (where the data
$\vy$ and the parameter vector $\vtheta$ are the same length, which never
occurs in our applications).
Here we solve this problem in the general case:
The inputs are multivariate (vectors) and the two Gaussians we are
multiplying live in spaces of different dimensions.
That is, we solve the following problem:

\paragraph{Problem:}
Find $N$-vector $\va$, $N\times N$ variance tensor $\tA$, $K$-vector $\vb$,
and $K\times K$ variance tensor $\tB$ such that
\begin{equation}\label{eq:problem}
\normal(\vy\given\mM\cdot\vtheta,\tC)\,\normal(\vtheta\given\vmu,\tLambda)
 = \normal(\vtheta\given\va,\tA)\,\normal(\vy\given\vb,\tB) \quad ,
\end{equation}
and such that $\va$, $\tA$, $\vb$, and $\tB$ don't depend on $\vtheta$ at all.
Note that
$\vy$ is a $N$-vector,
$\mM$ is a $N\times K$ matrix,
$\vtheta$ is a $K$-vector,
$\tC$ is a $N\times N$ non-negative semi-definite variance tensor,
$\vmu$ is a $K$-vector,
and
$\tLambda$ is a $K\times K$ non-negative semi-definite variance tensor.

\paragraph{Solution:}
\begin{equation}
\tA\inv = \tLambda\inv + \mM\T \cdot \tC\inv \cdot \mM
\end{equation}
\begin{equation}
\va = \tA \cdot (\tLambda\inv \cdot \vmu + \mM\T \cdot \tC\inv \cdot \vy)
\end{equation}
\begin{equation}
\tB = \tC + \mM \cdot \tLambda \cdot \mM\T
\end{equation}
\begin{equation}
\vb = \mM \cdot \vmu
\quad .
\end{equation}
This is the complete solution to the problem, and constitutes the main point
of this \documentname.
For completeness, we will give some discussion!

\paragraph{Proof:}
The two sides of equation~(\ref{eq:problem}) are identical if two things hold.
The first thing is that the determinant products must be equal:
\begin{equation}
||\tC||\,||\tLambda|| = ||\tA||\,||\tB||
\quad ,
\end{equation}
because the determinants are involved in the normalizations of the
functions.
This equality of determinant products follows straightforwardly from
the matrix determinant lemma
\begin{equation}\label{eq:detlemma}
||\tQ + \mU\cdot\mV\T|| = ||\tI + \mV\T\cdot\tQ\inv\cdot\mU||\,||\tQ||
\quad ,
\end{equation}
where $\mU$ and $\mV$ can be rectangular, and $\tI$ is the correct-sized identity matrix.
This identity implies that
\begin{equation}
||\tA\inv|| = ||\tI + \mM\T\cdot\tC\inv\cdot\mM\cdot\tLambda||\,||\tLambda\inv||
\end{equation}
\begin{equation}
||\tB||     = ||\tI + \mM\T\cdot\tC\inv\cdot\mM\cdot\tLambda||\,||\tC||
\quad,
\end{equation}
where we had to apply the identity twice to get the $||\tA\inv||$ expression.
We can ratio these as follows to prove this first thing:
\begin{equation}
||\tA||\,||\tB||
 = \frac{||\tB||}{||\tA\inv||}
 = \frac{||\tC||}{||\tLambda\inv||}
 = ||\tC||\,||\tLambda||
\quad .
\end{equation}

The second thing required for the proof is that the quadratic scalar form
\begin{equation}\label{eq:LHS}
[\vy-\mM\cdot\vtheta]\T\cdot\tC\inv\cdot[\vy-\mM\cdot\vtheta]
+ [\vtheta-\vmu]\T\cdot\tLambda\inv\cdot[\vtheta-\vmu]
\end{equation}
must equal the quadratic scalar form
\begin{equation}\label{eq:RHS}
[\vtheta-\va]\T\cdot\tA\inv\cdot[\vtheta-\va]
+ [\vy-\vb]\T\cdot\tB\inv\cdot[\vy-\vb]
\quad,
\end{equation}
because these quadratic scalar forms appear in the exponents in the functions.
This equality follows from straightforward expansion of
all the quadratic forms. ... HOGG

\paragraph{Solution notes:}
In principle we found this factorization by expanding the quadratic in
(\ref{eq:LHS}) and then completing the square.
Of course we didn't really; we used arguments (which physicists love)
called \emph{detailed balance}:
We required that the terms that look like
$\vtheta\T\cdot\tQ\cdot\vtheta$ were equal between the LHS~(\ref{eq:LHS})
and the RHS~(\ref{eq:RHS}), and then all the terms that look like
$\vmu\T\cdot\tS\cdot\vmu$, and so on.
It turns out you don't have to consider them all to get the right solution.

Because the matrix $\mM$ is not square, it has no inverse. And because this
is a physics problem, it has units (which are the units of $\dd\vy/\dd\vtheta$).
It's beautiful in the solution that $\mM$ and $\mM\T$ appear only where the
units make sense.
They make sense because the units of $\tC$ are inverse data-squared (where $\vy$
is the data vector) and the units of $\tLambda$ are inverse parameters-squared.
And they are different sizes.

Various parts of the solution are highly interpretable in terms of the
objects of Bayesian inference. For example, the vector $\va$ is the
maximum \foreign{a posteriori} (or \acronym{MAP}) value for the parameter vector
$\vtheta$.
It is found by inverse-variance-weigted combinations of the data and the prior.
The variance tensor $\tA$ is the posterior variance in the parameter space.
It is strictly non-larger (in eigenvalues or determinant) than either
the prior variance $\tLambda$ or the parameter-space data-noise
variance $\mM\T\cdot\tC\cdot\mM$.
The vector $\vb$ is the prior-optimal (maximum \foreign{a priori})
value for the data $\vy$.
It is the most likely data under the prior pdf.
And the variance tensor $\tB$ is the prior variance expanded out to the
data space, and including the noise variance in the data.
It is strictly non-smaller than both the data noise variance $\tC$ and the
data-space prior variance $\mM\cdot\tLambda\cdot\mM\T$.

HOGG: Coment on the possibility that there is a solution that flows
from thinking about the product of Gaussians as one Gaussian for the
joint. Does that lead to a simpler discussion?

\paragraph{Implementation notes:}
The solution gives an expression for the variance tensor $\tB$, but
note that when you actually evaluate the pdfs you probably need to
have either the inverse of $\tB$, or else an operator that computes
the product of the inverse and vectors, as in $\tB\inv\cdot\vy$ and
the same for $\vb$.
To get the inverse of the tensor $\tB$ stably, \emph{you might want to use
the matrix inversion lemma} (\ref{eq:invlemma}) given above.
This is often useful because you often know the data noise-variance
tensor inverse $\tC\inv$, and the prior variance inverse
$\tLambda\inv$.

We also give the general advice that one should \emph{never take an explicit
inverse} (unless you know the inverse exactly in closed form, as you do
for, say, diagonal tensors).
In your code you should never use the \code{inv()} function; you
should always use a \code{solve()} function.
The reason is that the code operation \code{inv(B)} returns the best
possible inverse to machine precision (if you are lucky), but what you
really want instead is the best possible product of that inverse times
a vector.
So in general \code{solve(B,y)} will deliver more precise results than
the mathematically equivalent \code{dot(inv(B),y)}.

The expressions in (\ref{eq:problem}) do not require that the tensors
$\tC$, $\tLambda$, $\tA$, $\tB$ be positive definite; they only require
that they be non-negative semi-definite.
That means that they can have zero eigenvalues.
As can their inverses $\tC\inv$, $\tLambda\inv$, $\tA\inv$, $\tB\inv$.
If either of these might happen in your problem---like if your prior
freezes the parameters to a subspace of the $\vtheta$-space, which
would lead to a zero eigenvalue in $\tLambda$, or if a data point is
unmeasured or missing, which would lead to a zero eigenvalue in
$\tC\inv$---you might have to \emph{think about how you implement the
linear algebra operations to be zero-safe}.

\section{Worked Example}

When working with a probabilistic model that meets the strong requirements
imposed above (Gaussians everywhere), the identities described in this
\documentname\ have a practical use: To reduce the dimensionality of your model.
This is often desired when your inferences depend on generating posterior
samples of your model parameters.
Reducing the dimensionality of your parameter-space will in general improve
convergence of Markov Chain Monte Carlo (MCMC) sampling methods, or enable
alternate sampling methods (e.g., rejection sampling) that may be intractable
when the parameter dimensionality is large.

Below, we demonstrate one way in which these identities can be used and
incorporated into statistical inferences TODO APW.
% One case where the parameter dimensionality is reduced, and another case where
% standard MCMC is intractable but the straightforward evaluation of the
% marginal likelihood enables using brute-force rejection sampling to generate
% posterior samplings.

APW: other notes as footnotes. (1) Like you wouldn't turn on scipy.optimize to get your maximum-likelihood linear parameters, don't turn on MCMC to generate samples if everything is linear and gaussian, (2) Another footnote or note about how this enables other sampling methods like The Joker.

APW: Example: Fit a spectral line + background. Reduced dimensionality from 5 to 2 parameters, run with emcee.


\end{document}
